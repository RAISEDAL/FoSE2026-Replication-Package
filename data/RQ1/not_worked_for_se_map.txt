192101946:Minimal impact on industry and almost no relevance to adjacent fields like AI. Core SE challenges—such as the oracle problem—remain unsolved or even ill-defined. The community suffers from excessive competition, unrealistic expectations for young researchers, and a broken peer-review system driven by incentives for high submission volume rather than quality. Senior scholars often exploit the system, producing dozens of papers per conference while contributing little to reviewing. The process is easy to game, overloaded with deadlines, and inflated by conference hype—particularly around ICSE—even though these venues hold little significance outside the SE bubble.
192120549:The community needs to bridge the gap between them and practitioners.
192161829:- we publish too much and too many low quality papers, e.g., in workshops, which is unsustainable; reviewing load is at an all time high and it also lessens the quality of the peer review process; the cracks in the system become more visible every month - submissions at our flagship conferences are geared towards certain types of papers because those are the hardest to reject, e.g., some AI approach that improves the current baseline; publishing studies with much higher industry or societal impact is much harder in comparison; it also doesn't help that the separate ICSE SEIP and SEIS tracks give the impression as if these types of papers are worth less than the main track and should not go to the main track - for-profit publishers but also some of our professional organizations like IEEE simply skim off too much money based on the unpaid labor of reviewers, editors, and conference organizers; while open access is desirable, it currently feels like a few companies and organizations are getting rich based on this
192205474:The circulation of knowledge beyond conferences remains limited. Talks and papers rarely reach practitioners or even researchers who couldn't attend, making the field feel niche and insular. Many presentations aren't recorded or shared, and papers are often inaccessible or difficult to digest for broader audiences. It would be great to have higher emphasis on communication quality, creating incentives for clearer, more engaging talks and more accessible dissemination formats that invite diverse audiences in.
192295956:I felt somewhat out of place at the ICSEs I attended, as it seemed that professors who regularly participate could easily connect with one another, while I, as a student, felt somewhat “out of their league” when trying to learn more. I believe that creating more financially accessible opportunities (especially for students) could help change this scenario. After all, in a few years, we will become the new generation of researchers, and this would make the transition smoother. Additionally, intentionally designing intergenerational interaction spaces would also be of great value. Another aspect is that, due to the nature of my research (which overlaps with education), I feel that opportunities are limited and that some members of the community still hold a somewhat distorted view of less traditional or less valued types of work. This has happened in reviews of papers I have submitted. It often conveys the message that “they don’t really understand what we’re doing” or “they don’t see our work as relevant.” In practice, this has led me to distance myself a bit from submitting to these venues, choosing instead to target conferences and journals focused on computing education, for example.
192305887:It seems that industry is often ahead of the research community, mostly due to bigger resources (personnel)
192308195:Too many conferences and too elitist. Pressure for publishing. Gap between research and practice.
192587927:The ICSE community, while vibrant and welcoming, is not as open to new ideas as one might expect when compared to conferences in other areas of computer science. This relative insularity has consequences: when engaging with researchers outside the software engineering community, we are often perceived as less relevant, less impactful, and overly focused on industrial applications. For example, the influence of the AI community on our work is far greater than our influence on theirs. Many of our innovations remain largely unknown outside software engineering, while we frequently adopt and adapt breakthroughs from other areas—such as supervised and unsupervised learning, NLP strategies, and, more recently, large language models. This raises a fundamental question: how can we position the ICSE community so that other research fields see us as influential and forward-looking, rather than merely consumers of external advances? We often express concerns about the relatively small size of subcommittees and program committees. However, we are not always proactive in inviting reviewers from outside our immediate field. Given the rapid rise of AI, for instance, there is a growing need to involve reviewers with expertise in AI and related areas. Expanding our reviewer base is just one step toward fostering greater openness and cross-pollination of ideas. Despite these challenges, our community is notably friendly, supportive, and full of untapped potential. By consciously improving the values we promote internally—such as inclusivity, recognition of innovation, and collaboration—we can significantly enhance our external visibility and competitiveness. Many other research communities are widely recognized for specific achievements, with LLMs being a clear example. In contrast, we have a wealth of success stories, both past and present, that are often under-communicated. Highlighting the novel approaches, automation tools, and influential software projects developed within our community would strengthen our profile. A lack of strong panels and high-visibility discussions further diminishes our ability to broadcast these achievements. Observing other conferences, it is clear they are highly effective at promoting their contributions and shaping their public image—something from which we could learn. Another area for improvement is participation in standardization efforts. Currently, our presence is minimal. With the growing impact of AI, there is a pressing need for our community to actively contribute to standardization discussions. Engaging in these efforts would not only elevate our relevance but also create valuable opportunities for joint exploration with industry and other research fields. More broadly, attending an SE conference should provide attendees with a clear understanding of: * What our community has achieved and how we can build upon these successes. * Where knowledge gaps and open challenges remain. * The impact goals we aim for in the future—not just reflecting on past achievements, but charting a vision for the next era of software engineering research. By addressing these points—embracing openness, celebrating our achievements, engaging in standardization, and clarifying our future impact—we can ensure that ICSE is recognized as a truly influential and forward-thinking community, both within computer science and beyond.
192588908:SE conferences and workshops are not including real experiences or real needs. And reseach papers are really endogamous. In addition, theoretical papers or studies of studies are interesting but they are not dealing with reality, with real problems in industrial settings, or carried out by practitioners. The so-called 'SE research community' is far from real issues and problems found by practitioners.
192597855:practitioners and researchers do not listen to each other
192626887:Too many papers and not enough value given to tools, application.
192676762:lack of influence on practice; few fundamental advances
192718035:- Polarization: Big conferences are getting bigger but small and medium size conferences are getting smaller. Competitions are too high for big conferences. Conferences was for supporting collective intelligence in solving common and urgent problems. Nowadays, it is more about showing off authors' achievements. - Lack of communication: Big conferences became formal ceremonies for rewarding authors. Presentations are too short to share behinds of the paper and discuss with audience. - review quality: Big conferences include broad areas and often failed to recruit enough number of experts for all areas. Therefore, your paper is highly likely to be reviewed by non-experts. Especially, interdisciplinary research is so common these days making almost everybody non-experts. - Data-oriented research: Theoretical and conceptual research is loosing grounds. - Overly specialized research: They work for specific dataset under strong assumptions about preconditions before applying the work. Each research is valuable, but they are scattered, isolated and hard to integrate. The gap between research and practice is getting larger and larger.
192723628:Influencing the mainstream of software engineering with our research, the hot topics are set by practitioners outside of the research community
193668599:Can be cliques of researchers sticking together; too much pressure to publish; all papers on LLMs with lack of transparency and guidelines for research methods; unfair reviews that make it seem random to get accepted at top conferences; lack of knowledge transfer to practice and real-world impact
193757741:Tech transfer and building partnerships between industry and research. While it happens sometimes (particularly in the context of industrial research labs) and there is an important role for fundamental research not intended to be immediately practical, there's not enough emphasis on relevance to real practice for SE research intended to be practical (e.g., studies of practice, developer focused tools). This seems largely due to most in industry, even and especially those designing new developer tools or offering advice on practice, having little interest in the work SE researchers do.
193795120:The uncertain problems like software architecture, functional requirements, process and product modelling and various non-functional properties (security, privacy, scalability, etc.) appear to be largely explored in isolation, and are not seamlessly integrated into SDLC. Ofcourse, there are explorations in areas like privacy and security by design, but they seem to fall short of being integratable in the end-to-end SDLC.
194004592:Research is often irrelevant for practice
195155159:The applicability of the research to solutions that effectively work for society
195289566:We have lost sight of the discipline’s strategic role. Software engineering is treated as a supportive technology for frontier technologies like AI, quantum, photonics or cybersecurity, not as the enabling science that makes these fields possible. This leads to fragmented research agendas, short-term projects, and underrepresentation in policy and funding. The pressure to publish quantity over substance amplifies that fragmentation.
195756280:I guess we lack the ability to coordinate ourselves around relevant phenomena/outcomes of interest for practitioners to truly build helpful bodies of evidence. Therefore, we endup with lots of systematic mapping studies that are great to support our own research, but with few true systematic literature reviews that could be converted on recommendations/guidelines or even advice to practitioners.
195756655:Way out of hand from industry. Codex/copilot can eliminate the novelty of more than half of the paper’s idea
195771332:Conference publication process seems broken: deadlines are clustered, feedback is often negative and random, hybrid conferences don't work well. Too much navel gazing in research conducted and not enough focused on translating results to industry and impact.
195775809:Many recent SE studies rely on LLMs, but their high computational cost makes it difficult for researchers without industry collaboration to run large-scale experiments. As a result, companies with extensive GPU resources gain disproportionate influence over the direction and reproducibility of SE research.
195789338:There is a great gap between academic research and practice in industry. The reason is that reviewers are usually academics and require strong theoretical discussions. Crazely, many young reviwers even require that every work be evaluated by empirical study. This is crazing because SE is not a natural science and every experiment is not repeatable. Nobody believes the results of experiments because of so many uncertain factors affecting the results. The best way is to require researchers to present a case study to demonstrate the process of applying the proposed technologoy. Thus, readers will have their own judgement in adopting the proposal. Also, ICSE has so many PC members from the same country that affect the quality of reviews of papers.
195832864:Analysis of practical implications, i.e., if it works in practice or not
195853386:Most of the published SE research is irrelevant for practitioners, and this got worse over the past years. “We improved x% over a random and toy baseline” is not something practitioners care about. Also, a lot of recent SE research is hype-driven, and I miss the scientific rigor and distance required for an A* conference such as ICSE. “We have thrown an LLM at a problem, and it mostly worked” is not research. Also, we're still organizing conferences in countries that exclude people from certain regions to attend, while we talk a lot about diversity and inclusion at these conferences. The option to, e.g., present an ICSE paper at the following FSE or ASE is a first step toward mitigating this (event though it's just a workaround). Our conferences are still very much focused on North America and Europe, while a lot of innovation is happening in Asia at the moment.
195860877:The ramp up of Rigour is too slow, leading to research that looks too similar to industry: if it works, it's valid. We are not competing with industry, we are doing something different than them, and we often forget this.
195868073:A decently high percentage of the published work is completely detached from practitioner reality. I suspect the reason for this is two-fold: (1) incentives are in the wrong place (publishing a lot of useless papers is 'better' for a career than a few high-impact ones), (2) a decently high number of SE academics have no idea what is actually done in practice. The use of paid publishers reduces visibility and costs a fortune for very little value-add. I think we could take a lesson from colleagues in the AI community who have gone with a publishing model that is directly linked to the conferences. This allows them to still get high-quality peer review (which the academics provide for free anyway), while having open access to papers without paying through the nose.
195910267:Lots of gatekeepers. Looking down their nose at industry practitioners. Most SE researchers these days have zero industry experience to begin with, so it's hard for them to choose research that is both useful to industry and interesting to academia.
195953547:There is a major disjoint between what is researched and what is useful to be used in practice.
195962364:Hard to break into, not always representative of the actual work software engineers do in the “real world”
195984732:1) Overemphasis on novelty at the expense of rigor. Novelty and impact are explicit evaluation criteria in every call for papers for our major conferences, and perhaps we should question that again. 2) Limited diversity in methods (which ultimately leads to a lack of expertise among reviewers and thus to a lot of frustration). 3) Reviewing fatigue: too many submissions handled by too few people.
195992490:Endless reviewing and a lack of industrial relevance, especially because SE is an applied discipline
195999111:(1) general focus on high paper count, which rewards quantity over quality; this then also results in more incremental work and more reviewing (2) deadlines and dates for the major conferences: a few years back, it was great to have one in fall (FSE) and one in spring/early summer (ICSE) with two deadlines per year roughly half a year apart. Now the deadlines are not really synced anymore and the ICSE event overlaps exactly with CHI, which is problematic for people that publish in both venues (3) Transfer from research to industry; in some fields there is translational research that people work on just to bring research insights into practice, which could be interesting; and despite the fact that our research is often close to practice (and not as far away as biomedical science from drug development), we often do not manage to transfer the findings into practice and achieve more impact
196009614:Communication to the outside world
196010414:Working in silo
196022630:We probably need to publish less! And publish higher-quality impactful research. There is a long- term disconnect with industrial practice, and that practice has tended to out-pace the research community.
196030130:The software engineering community is still very result-driven (focus on the number of publications and awards) and less focus on tool development (despite having a software artifact track, not many of the software artifacts are being actively maintained). This may not work well for software tool developers who work hard to design and maintain tools. Another example of result-driven is that software testing papers always judge the effectiveness of a tool by the number of bugs reported or fixed by developers. This provides a lot of pressure to submit bug reports and some developers may not appreciate the bug reports detected using automated tools.
196032564:Application research, interdisciplinary research, and and-to-end case studies. Narrowing the evaluation of a stellar ICSE research paper so that the evaluation becomes unassailable is detrimental to software engineering.
196061190:It fails to produce more influential work and new techniques.
196061195:However, the impact our SE community has (to other fileds, e.g., AI/NLP) are limited, compared to other fields to SE.
196065027:Work with practical relevance is undervalued, i.e., in separate tracks at conferences and journals that are valued less (treated as second class citizens)
196065742:While many niche conferences tend to be quite small (discussion foras), large conferences like ICSE tend to have biased selection of particular type of research papers (e.g., technical studies with large datasets favored at ICSE). Lack of industry participants in many conferences (with exception of XP, for example). Insufficient attention to theory despite a few fundamental methodological guidance articles published in the last decade.
196067582:By comparing to other fields, I think we are too critical to our colleagues research in this sense that we are reluctant to accept papers that replicate previous study, or just provide evidence in the areas that were already studied (sometimes having single studies only) as we believe these are not new results. The same relates to replications - see medical sciences as counter example - they publish single cases as they allow to build body of konowledge in the long run. Also, we are very picky on what improvement is - look at ML field - they have a race on improving things - we expected only novel ideas. I think this makes SE research body of knoweldge very shallow and with little relevancy to the industrial practice...
196069944:The high pressure to publish in a small number of top-tier venues often leads to an incremental 'least publishable unit' culture, discouraging risky, long-term, and potentially high-impact foundational research.
196070836:Despite close ties with industry in some areas, much SE research still struggles to have a tangible impact on industrial practice. The “publish or perish” culture places heavy emphasis on publication counts and conference rankings rather than sustained, high-quality contributions. Although open science is improving, reproducibility is still uneven across the community. The SE community is large and diverse, but that diversity can sometimes lead to fragmentation. The community is still struggling to achieve equitable representation across gender, geography, and economic regions. Many research projects are short-term, driven by grants that last 2–3 years, which limits continuity.
196071446:We still have very little meaningful industry presence at most SE conferences. It feels like we talk about industry all the time, but actual practitioners rarely attend. The publication system can be pretty frustrating. The lack of coordination between journals and conferences leads to the same papers being submitted over and over again until they get accepted somewhere. It wastes time for everyone involved and adds unnecessary noise. Another big issue is that conference venue decisions don’t take visa restrictions seriously. D&I efforts talk a lot about gender and other dimensions but rarely address this, even though it has a big impact. The current North America / Europe / rest of the world model is not an accurate representation of where authors and participants are from.
196071861:Those that are intrinsic of SE, such as software architecture, software quality, design patterns, object orientation, new programming languages, programming in general, DevOps or strategies used in the industry are considered so naive for the academia.
196073315:1) Big gap between research done in Industry and Academy. Strangely enough (as Software industry is the most richest one nowadays, and where software is absolutely essential in any domain we might think of), there is no practice of the SE industry to support research like it happens to other scientific areas (Medicine, Pharmaceutical, ...) . 2) Publication of papers in non-open access places (the SE community embraced the business logic of publishers, who have an outrageous business model, that is against the 'science for all' principle); 3) Obsession with the publication metrics that make it hard for researchers to concentrate on doing deep work as they are pressed to show 'productivity'; 4) SE research is becoming more and more for the rich groups/countries that have the money to pay for all sort of services to be more productive and support their research. 5) Conferences are no longer places for networking and brainstorming new ideas; they are just places for dumping results and passively attending to talks. 6) Conferences like ICSE are insanely expensive for some of us (it represents more than a month of my salary to be able to afford accomodation+transportation+registration).
196073866:I find reviewing practices in the software engineering community too picky, and in particular in the software testing one. The reviewing workload in conferences is extremely high and therefore my feeling is that reviewers do not even read the paper. This converts the process into an entirely random process. The impact of our research has become very limited if we compare to other CS communities, such as AI or Robotics.
196075893:The general public starts to think that there is little need for software engineering/programming/... since bots will take over; the SE research community couldn't contain that view yet.
196078058:1. Many (if not most) researchers appear to pay little attention to the actual engineering relevance of their work. For instance, very few tool builders appear to be interested in evaluating actual usefulness (as opposed to applying objective metrics in an automated fashion). This goes on even within the measurements, where I often see an indiscriminate mix of useful and misleading metrics, with no discussion. 2. We publish a lot at conferences, which have the wonderful property of a discussion period in between reviewing and acceptance decision. Much too little change to reviews and verdicts is going on in these discussion periods.
196082256:A little weak promotion of good works to broader communities
196091544:Too far from practice.
196098713:Many (most?) papers do not address key Software Engineering problems. Requirements, IDEs, Visualization, and Debugging are examples of extremely important concerns in SE poorly represented. Often the focus is higher on the solution space (in defense of novelty) as opposed to the problem.
196119370:The community can also be problematic: focused on particular kinds/topics/methods of research (and judgmental or dismissive of others), tied to norms (or to particular methods, practices, notations) in ways that create bias, prone to 'error by proxy', unresponsive to communications ... It can also be difficult to form effective relationships between academia and industry, to connect to current industry concerns, and so on.
196144420:There is maybe too much focus on publishing papers, and moving to the next hot topic, instead of ensuring reproducibility, building platforms to facilitate future research, or revisit topics from new points of view.
196176584:much hype-driven research, lack of theory, lack of thoroughness in study designs, lack of studies of phenomena over time. Many superficial literature reviews. Lack of impact on practice.
196181126:Connecting research and practice. We both need to participate more into industry forums and to include them in ours academic ones. It is not about invitation, but actually a deeper synergy. Also, research circles are often uninviting unless funding is a goal.
196183707:The dissemination to the research to society
196208899:Shift toward social aspects, which I do not think most of the software engineering researchers are competent on; too much emphasis on observing actual software engineers rather than on producing new contributions that could help them in their work; cumbersome review processes in conferences, which do not seem to result in significant advancements in the quality of accepted papers
196248189:Reviews are often unclear / random. - Both the expectation reviewers have are arbitrary (while many focus on what is wrong or not), also many focus on: do they like it? Is it well written, etc. - In case double rounds are more in-depth discussions are planned, often people do not react or care. As a result overall reviewing and acceptance is to a large extend random. Contentwise: most ideas that influence our field are not coming from SE research. - SE research is mostly a follower, not an innovator. (e.g., agile, micro-services, devops, etc.) Even AI research innovations have a larger impact on SE (research and practice) than most from SE.
196248869:Transfer to industry
196290621:Convincing industry that the research is relevant for practitioners
196290840:low interest of papers, non-sustainable system of international conferences, most industry practices do not come from software engineering research.
196293762:Software design incorporates 'bad' thinking incoming from controversial user requirements. Socially unsustainable software such as those for military applications or smart farming delivers the worst aspects of humanity. The software engineering community has not taken a position on its role and responsibility in spreading controversial arguments. The ethics of software engineering lacks authoritative avenues of discussion, and the community itself hypocritically uses the topic of ethic just for increasing the number of publications.
196368894:Too many papers published lack implementations available on open-source repositories for reproducibility and reuse. Too many papers published evaluate the ideas they put forward on toy case studies rather than scale-realistic case studies coming from genuine industrial needs. Too many software engineering research tools do not practice in their own implementation the engineering principles they purport to support.
196380886:Excessive focus on 'novelty' that encourages publishing very low-impact work (e.g., all kinds of 'AI for X' low-hanging fruits), and aversion to work related to engineering/scalability/applicability that makes more real-world impact without necessarily inventing a new wheel.
196389340:producing practical, impactful methods
196391274:A large portion of work in ML4Code, defect prediction, and program analysis continues to overfit small or outdated datasets (such as Defects4J), which limits the real-world generalizability of reported results. At the same time, many SE papers introduce solutions that rarely see adoption in practice, including overly complex static analyses, heavyweight transformation tools, and niche testing frameworks. Across major venues, the incentive structure often favors incremental algorithmic novelty over practical relevance, even though industry impact would benefit far more from robustness, reproducibility, and engineering insights. In addition, conference participation remains challenging for researchers from certain countries due to stringent visa requirements, which can hinder diversity, collaboration, and the overall inclusiveness of the SE community.
196573064:The integration of the academia and the industry. This is the main barrier to many studies to be adopted. The academia has a focus on publishing and the industry in delivering software. So, most studies demmand too much to be done, while the industry (mainly small-mid software companies) does not have this time and effort to be spend.
196579032:(a) Lack of practical impacts, especially on industry practices, (b) overlooking the challenges that industry might be interested in (e.g., scalability, load balancing), and (c) lack of impacts on the general public, unlike other areas like AI.
